{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lestermartin/starburst-dataframes-exploration/blob/main/IcebergMigrationTool/Migrate2Iceberg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3883d2a",
      "metadata": {
        "id": "c3883d2a"
      },
      "source": [
        "# Starburst Galaxy Iceberg migration tool\n",
        "\n",
        "An interactive notebook used to migrate non-Iceberg tables in a given\n",
        "Starburst Galaxy data lake catalog.\n",
        "\n",
        "See [Migrate Hive tables to Apache Iceberg with Starburst Galaxy tutorial](https://www.starburst.io/tutorials/migrate-hive-tables-to-iceberg-with-starburst-galaxy/#0)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd8adb40",
      "metadata": {
        "id": "bd8adb40"
      },
      "source": [
        "---\n",
        "## Config & setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pystarburst"
      ],
      "metadata": {
        "id": "o2b5GfLn3zFQ"
      },
      "id": "o2b5GfLn3zFQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w3Zm9a9_3z3H"
      },
      "id": "w3Zm9a9_3z3H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "11b425ab",
      "metadata": {
        "id": "11b425ab"
      },
      "source": [
        "### Galaxy cluster & user credentials\n",
        "\n",
        "Run the next cell, but realize that it does NOT actually validate your values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "999ddb74",
      "metadata": {
        "id": "999ddb74"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "\n",
        "# grab credentials from the notebook user to be used when making a connection\n",
        "host = input(\"Host name\")\n",
        "username = input(\"User name\")\n",
        "password = getpass.getpass(\"Password\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69f92b65",
      "metadata": {
        "id": "69f92b65"
      },
      "source": [
        "### Migration process parameters\n",
        "\n",
        "Update the values below to the most appropriate values for your migration effort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f97beb1",
      "metadata": {
        "id": "9f97beb1"
      },
      "outputs": [],
      "source": [
        "# Galaxy target catalog to perform migration on\n",
        "tgt_cat = 'mycatalog'\n",
        "\n",
        "# schema to target migration effort on\n",
        "tgt_sch = 'myschema'\n",
        "#TODO: allow '*' to be valid and used to loop through all schemas in tgt_cat\n",
        "\n",
        "# CTAS properties used in WITH clause on new tables created for shadow migration\n",
        "with_props = \"type='iceberg', format='parquet'\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05f55111",
      "metadata": {
        "id": "05f55111"
      },
      "source": [
        "### Setup PyStarburst session\n",
        "\n",
        "Should return `[Row(Working='Yes')]` if functional.  If an exception is raised,\n",
        "it is likely due to incorrect cluster and/or credentials values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97d31b94",
      "metadata": {
        "id": "97d31b94"
      },
      "outputs": [],
      "source": [
        "import trino\n",
        "\n",
        "from pystarburst import Session\n",
        "from pystarburst import functions as F\n",
        "from pystarburst.functions import *\n",
        "from pystarburst.window import Window as W\n",
        "\n",
        "# PyStarburst setup\n",
        "session_properties = {\n",
        "    \"host\":host,\n",
        "    \"port\": 443,\n",
        "    \"http_scheme\": \"https\",\n",
        "    \"auth\": trino.auth.BasicAuthentication(username, password)\n",
        "}\n",
        "session = Session.builder.configs(session_properties).create()\n",
        "\n",
        "# validate PyStarburst working\n",
        "session.sql(\"select 'Yes' as Working\").collect()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session.sql(\"SELECT * FROM system.runtime.nodes\").collect()"
      ],
      "metadata": {
        "id": "v4T4_G5f4vmF"
      },
      "id": "v4T4_G5f4vmF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6d8c4d62",
      "metadata": {
        "id": "6d8c4d62"
      },
      "source": [
        "---\n",
        "## Initial analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "313d1add",
      "metadata": {
        "id": "313d1add"
      },
      "source": [
        "### Identify targeted tables\n",
        "\n",
        "Fully qualified table names of all tables that will be interogated and attempted to be migrated if appropriate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c3c98b2",
      "metadata": {
        "id": "5c3c98b2"
      },
      "outputs": [],
      "source": [
        "# get all BASE TABLE entries in the info schema's tables table\n",
        "table_list = session \\\n",
        "    .table(tgt_cat + \".information_schema.tables\") \\\n",
        "    .filter(\"table_schema = '\" + tgt_sch + \"' AND table_type = 'BASE TABLE'\") \\\n",
        "    .select_expr(\"table_catalog||'.'||table_schema||'.'||table_name as table_name\") \\\n",
        "    .collect()\n",
        "\n",
        "for a_table in table_list:\n",
        "    print(a_table.table_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e463cfb7",
      "metadata": {
        "id": "e463cfb7"
      },
      "source": [
        "### Categorize tables\n",
        "\n",
        "Identify tables for each of the following categories.\n",
        "\n",
        "- Existing Iceberg tables (no action to take)\n",
        "- Hive tables backed by ORC, Parquet, or Avro (will attempt in-place migrations via ALTER command)\n",
        "- Hive tables backed by other file formats (will attempt shadow migrations via CTAS statement)\n",
        "- Non Hive or Iceberg tables (doing nothing with these at this time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7e39614",
      "metadata": {
        "id": "e7e39614"
      },
      "outputs": [],
      "source": [
        "# create some lists to separate the table types into\n",
        "hive_tables_2rewrite = list()\n",
        "hive_tables_2migrate = list()\n",
        "iceberg_tables = list()\n",
        "other_tables   = list()\n",
        "\n",
        "# look at the table create statement to determine which category to use\n",
        "for a_table in table_list:\n",
        "    table_name = a_table.table_name\n",
        "    crt_tb = session.sql('show create table ' + table_name).collect()\n",
        "    cts = crt_tb[0][\"Create Table\"]\n",
        "\n",
        "    if \"type = 'HIVE'\" in cts:\n",
        "        if \"format = 'PARQUET'\" in cts or \"format = 'ORC'\" in cts or \"format = 'AVRO'\" in cts:\n",
        "            hive_tables_2migrate.append(table_name)\n",
        "        else:\n",
        "            hive_tables_2rewrite.append(table_name)\n",
        "    elif \"type = 'ICEBERG'\" in cts:\n",
        "        iceberg_tables.append(table_name)\n",
        "    else:\n",
        "        other_tables.append(table_name)\n",
        "\n",
        "\n",
        "print(\"\\n\" + str(len(iceberg_tables)) + \" Iceberg tables already exist -- no action will be taken on these...\")\n",
        "print(iceberg_tables)\n",
        "\n",
        "print(\"\\n\" + str(len(hive_tables_2migrate)) + \" Hive tables targeted to be MIGRATED (in-place) to Iceberg...\")\n",
        "print(hive_tables_2migrate)\n",
        "\n",
        "print(\"\\n\" + str(len(hive_tables_2rewrite)) + \" Hive tables targeted to be REWRITTEN (ctas) to Iceberg...\")\n",
        "print(hive_tables_2rewrite)\n",
        "\n",
        "print(\"\\n\" + str(len(other_tables)) + \" non Iceberg or Hive tables exist -- no action will be taken on these \" + \\\n",
        "      \"(BUT Delta Lake TABLES COULD BE REWRITTEN)...\")\n",
        "print(other_tables)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "701f59de",
      "metadata": {
        "id": "701f59de"
      },
      "source": [
        "---\n",
        "## Begin migration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a90a7114",
      "metadata": {
        "id": "a90a7114"
      },
      "source": [
        "### Perform in-place migrations\n",
        "\n",
        "Run `ALTER TABLE table_name SET PROPERTIES type = 'ICEBERG'` on Hive tables backed by ORC, Parquet, or Avro file format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02809ab6",
      "metadata": {
        "id": "02809ab6"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\")\n",
        "print(\"+++++++++++++++++++++++++++++\")\n",
        "print(\"++++ IN-PLACE MIGRATIONS ++++\")\n",
        "print(\"+++++++++++++++++++++++++++++\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# NEED TO HANDLE ANY EXEPTIONS THAT MIGHT BE RAISED\n",
        "#  these would likely be caused from invalid data types for starters\n",
        "#  which once identified could be added to the shadow migration list\n",
        "\n",
        "for tbl in hive_tables_2migrate:\n",
        "    print(\"in-place migration > \" + tbl)\n",
        "    session.sql(\"ALTER TABLE \" + tbl + \" SET PROPERTIES type = 'ICEBERG'\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eae5f327",
      "metadata": {
        "id": "eae5f327"
      },
      "source": [
        "### Perform shadow migrations\n",
        "\n",
        "Perform the following steps for Hive tables backed by file formats not supported directly by Iceberg.\n",
        "\n",
        "- `ALTER TABLE table_name RENAME TO hold_name`\n",
        "- `CREATE TABLE table_name WITH (with_props) AS SELECT * FROM hold_name`\n",
        "- `ALTER TABLE hold_name RENAME TO rm_name`\n",
        "- Add `rm_name` to a collection to later be dropped, if desired"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dea61d52",
      "metadata": {
        "id": "dea61d52"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\")\n",
        "print(\"+++++++++++++++++++++++++++++\")\n",
        "print(\"++++ SHADOW MIGRATIONS ++++++\")\n",
        "print(\"+++++++++++++++++++++++++++++\")\n",
        "print(\"+++ with_props > \" + with_props)\n",
        "print(\"+++++++++++++++++++++++++++++\")\n",
        "print(\"\\n\")\n",
        "\n",
        "old_tbls2rm = list()\n",
        "\n",
        "for tbl in hive_tables_2rewrite:\n",
        "    print(\"******** shadow migration > \" + tbl + \"\\n\")\n",
        "    fqtn = tbl.split(\".\")\n",
        "    hold_name = fqtn[0]+\".\"+fqtn[1]+\".hold_\"+fqtn[2]\n",
        "    alter2hold_cmd = \"ALTER TABLE \" + tbl + \" RENAME TO \" + hold_name\n",
        "    print(alter2hold_cmd)\n",
        "    session.sql(alter2hold_cmd).show()\n",
        "\n",
        "    # NEED TO TACKLE PARTITIONS BY LOOPING THROUGH THEM INSERTING FROM MOST RECENT TO LEAST RECENT\n",
        "    #  could even create a union with the old and new table then once new partition is committed,\n",
        "    #  quickly drop the old partition (briefly have 2 copies of each partition!!)\n",
        "\n",
        "    ctas_cmd = \"CREATE TABLE \" + tbl + \" WITH (\" + with_props + \") AS SELECT * FROM \" + hold_name\n",
        "    print(ctas_cmd)\n",
        "    session.sql(ctas_cmd).show()\n",
        "\n",
        "    fqtn = hold_name.split(\".\")\n",
        "    rm_name = fqtn[0]+\".\"+fqtn[1]+\".rm_\"+fqtn[2]\n",
        "    alter2rm_cmd = \"ALTER TABLE \" + hold_name + \" RENAME TO \" + rm_name\n",
        "    print(alter2rm_cmd)\n",
        "    session.sql(alter2rm_cmd).show()\n",
        "\n",
        "    # hold on to the rm_name for possible deletions later\n",
        "    old_tbls2rm.append(rm_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e85def2e",
      "metadata": {
        "id": "e85def2e"
      },
      "source": [
        "### Optionally, delete migrated tables\n",
        "\n",
        "If desired, run `DROP TABLE` commands on the original Hive tables that were migrated with the shadow (i.e. CTAS) approach.\n",
        "\n",
        "NOTE: Their names are prefixed with `rm_hold_`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f965fe5",
      "metadata": {
        "id": "9f965fe5"
      },
      "outputs": [],
      "source": [
        "# cleanup of the original tables that were shadow migrated\n",
        "\n",
        "for tbl in old_tbls2rm:\n",
        "    print(\"\\ndropping original table > \" + tbl)\n",
        "    session.sql(\"DROP TABLE \" + tbl).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2ae11af",
      "metadata": {
        "id": "f2ae11af"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}