{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lestermartin/starburst-dataframes-exploration/blob/main/IcebergMigrationTool/Migrate2Iceberg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3883d2a",
      "metadata": {
        "id": "c3883d2a"
      },
      "source": [
        "# Starburst Galaxy Iceberg migration tool\n",
        "\n",
        "An interactive notebook used to migrate non-Iceberg tables in a given\n",
        "Starburst Galaxy data lake catalog.\n",
        "\n",
        "See [Migrate Hive tables to Apache Iceberg with Starburst Galaxy tutorial](https://www.starburst.io/tutorials/migrate-hive-tables-to-iceberg-with-starburst-galaxy/#0)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd8adb40",
      "metadata": {
        "id": "bd8adb40"
      },
      "source": [
        "---\n",
        "## Config & setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pystarburst"
      ],
      "metadata": {
        "id": "o2b5GfLn3zFQ",
        "outputId": "0423913c-8b5f-4674-deb4-da6de9fe551b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "o2b5GfLn3zFQ",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pystarburst\n",
            "  Downloading pystarburst-0.9.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from pystarburst) (2.11.7)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pystarburst) (2.9.0.post0)\n",
            "Collecting trino<0.330.0,>=0.329.0 (from pystarburst)\n",
            "  Downloading trino-0.329.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: urllib3<3.0.0,>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from pystarburst) (2.5.0)\n",
            "Collecting zstandard<0.23.0,>=0.22.0 (from pystarburst)\n",
            "  Downloading zstandard-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->pystarburst) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->pystarburst) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->pystarburst) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->pystarburst) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.8.2->pystarburst) (1.17.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from trino<0.330.0,>=0.329.0->pystarburst) (2025.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from trino<0.330.0,>=0.329.0->pystarburst) (2.32.3)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.11/dist-packages (from trino<0.330.0,>=0.329.0->pystarburst) (5.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->trino<0.330.0,>=0.329.0->pystarburst) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->trino<0.330.0,>=0.329.0->pystarburst) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->trino<0.330.0,>=0.329.0->pystarburst) (2025.7.14)\n",
            "Downloading pystarburst-0.9.0-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trino-0.329.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zstandard-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: zstandard, trino, pystarburst\n",
            "  Attempting uninstall: zstandard\n",
            "    Found existing installation: zstandard 0.23.0\n",
            "    Uninstalling zstandard-0.23.0:\n",
            "      Successfully uninstalled zstandard-0.23.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langsmith 0.4.7 requires zstandard<0.24.0,>=0.23.0, but you have zstandard 0.22.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pystarburst-0.9.0 trino-0.329.0 zstandard-0.22.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w3Zm9a9_3z3H"
      },
      "id": "w3Zm9a9_3z3H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "11b425ab",
      "metadata": {
        "id": "11b425ab"
      },
      "source": [
        "### Galaxy cluster & user credentials\n",
        "\n",
        "Run the next cell, but realize that it does NOT actually validate your values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "999ddb74",
      "metadata": {
        "id": "999ddb74",
        "outputId": "e31cb4a9-c2a3-4bc8-f5ad-f83e5c49794c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Host nametrain-aws-us-east-1-small.trino.galaxy.starburst.io\n",
            "User namelester.martin@starburstdata.com/accountadmin\n",
            "Password··········\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "\n",
        "# grab credentials from the notebook user to be used when making a connection\n",
        "host = input(\"Host name\")\n",
        "username = input(\"User name\")\n",
        "password = getpass.getpass(\"Password\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69f92b65",
      "metadata": {
        "id": "69f92b65"
      },
      "source": [
        "### Migration process parameters\n",
        "\n",
        "Update the values below to the most appropriate values for your migration effort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9f97beb1",
      "metadata": {
        "id": "9f97beb1"
      },
      "outputs": [],
      "source": [
        "# Galaxy target catalog to perform migration on\n",
        "tgt_cat = 'mycatalog'\n",
        "\n",
        "# schema to target migration effort on\n",
        "tgt_sch = 'myschema'\n",
        "#TODO: allow '*' to be valid and used to loop through all schemas in tgt_cat\n",
        "\n",
        "# CTAS properties used in WITH clause on new tables created for shadow migration\n",
        "with_props = \"type='iceberg', format='parquet'\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05f55111",
      "metadata": {
        "id": "05f55111"
      },
      "source": [
        "### Setup PyStarburst session\n",
        "\n",
        "Should return `[Row(Working='Yes')]` if functional.  If an exception is raised,\n",
        "it is likely due to incorrect cluster and/or credentials values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "97d31b94",
      "metadata": {
        "id": "97d31b94",
        "outputId": "e22de8ce-597c-44e3-a487-947ecae8f0df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(Working='Yes')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import trino\n",
        "\n",
        "from pystarburst import Session\n",
        "from pystarburst import functions as F\n",
        "from pystarburst.functions import *\n",
        "from pystarburst.window import Window as W\n",
        "\n",
        "# PyStarburst setup\n",
        "session_properties = {\n",
        "    \"host\":host,\n",
        "    \"port\": 443,\n",
        "    \"http_scheme\": \"https\",\n",
        "    \"auth\": trino.auth.BasicAuthentication(username, password)\n",
        "}\n",
        "session = Session.builder.configs(session_properties).create()\n",
        "\n",
        "# validate PyStarburst working\n",
        "session.sql(\"select 'Yes' as Working\").collect()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session.sql(\"SELECT * FROM system.runtime.nodes\").collect()"
      ],
      "metadata": {
        "id": "v4T4_G5f4vmF",
        "outputId": "1415bf44-177c-45b6-8201-b2752904fe44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "v4T4_G5f4vmF",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(node_id='trino-worker-56fb5497-8pv85', http_uri='http://10.5.125.147:8080', node_version='476-galaxy-1-u136-g710558245c7', coordinator=False, state='active'),\n",
              " Row(node_id='trino-worker-56fb5497-bclpb', http_uri='http://10.5.19.39:8080', node_version='476-galaxy-1-u136-g710558245c7', coordinator=False, state='active'),\n",
              " Row(node_id='trino-worker-56fb5497-g92g2', http_uri='http://10.5.85.205:8080', node_version='476-galaxy-1-u136-g710558245c7', coordinator=False, state='active'),\n",
              " Row(node_id='trino-coordinator-5bb5959994-zgdpk', http_uri='http://10.5.54.157:8080', node_version='476-galaxy-1-u136-g710558245c7', coordinator=True, state='active'),\n",
              " Row(node_id='trino-worker-56fb5497-gkdfz', http_uri='http://10.5.18.139:8080', node_version='476-galaxy-1-u136-g710558245c7', coordinator=False, state='active')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d8c4d62",
      "metadata": {
        "id": "6d8c4d62"
      },
      "source": [
        "---\n",
        "## Initial analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "313d1add",
      "metadata": {
        "id": "313d1add"
      },
      "source": [
        "### Identify targeted tables\n",
        "\n",
        "Fully qualified table names of all tables that will be interogated and attempted to be migrated if appropriate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5c3c98b2",
      "metadata": {
        "id": "5c3c98b2",
        "outputId": "afea3768-4681-43de-9e6d-e33df6f93008",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "students.hive2ice.tpch_nation_hive_textfile\n",
            "students.hive2ice.tpch_orders_hive_orc\n",
            "students.hive2ice.tpch_orders_hive_json\n",
            "students.hive2ice.tpch_cust_ice_orc\n",
            "students.hive2ice.tpch_cust_hive_orc\n",
            "students.hive2ice.tpch_orders_delta\n",
            "students.hive2ice.tpch_orders_hive_textfile\n",
            "students.hive2ice.tpch_cust_ice_avro\n",
            "students.hive2ice.tpch_orders_hive_parquet\n",
            "students.hive2ice.tpch_cust_hive_textfile\n",
            "students.hive2ice.tpch_orders_ice_orc\n",
            "students.hive2ice.tpch_orders_hive_avro\n",
            "students.hive2ice.tpch_cust_hive_avro\n",
            "students.hive2ice.tpch_cust_delta\n",
            "students.hive2ice.tpch_cust_ice_parquet\n",
            "students.hive2ice.tpch_orders_ice_avro\n",
            "students.hive2ice.tpch_orders_ice_parquet\n",
            "students.hive2ice.tpch_cust_hive_parquet\n",
            "students.hive2ice.tpch_cust_hive_json\n",
            "students.hive2ice.tpch_nation_hive_json\n"
          ]
        }
      ],
      "source": [
        "# get all BASE TABLE entries in the info schema's tables table\n",
        "table_list = session \\\n",
        "    .table(tgt_cat + \".information_schema.tables\") \\\n",
        "    .filter(\"table_schema = '\" + tgt_sch + \"' AND table_type = 'BASE TABLE'\") \\\n",
        "    .select_expr(\"table_catalog||'.'||table_schema||'.'||table_name as table_name\") \\\n",
        "    .collect()\n",
        "\n",
        "for a_table in table_list:\n",
        "    print(a_table.table_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e463cfb7",
      "metadata": {
        "id": "e463cfb7"
      },
      "source": [
        "### Categorize tables\n",
        "\n",
        "Identify tables for each of the following categories.\n",
        "\n",
        "- Existing Iceberg tables (no action to take)\n",
        "- Hive tables backed by ORC, Parquet, or Avro (will attempt in-place migrations via ALTER command)\n",
        "- Hive tables backed by other file formats (will attempt shadow migrations via CTAS statement)\n",
        "- Non Hive or Iceberg tables (doing nothing with these at this time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e7e39614",
      "metadata": {
        "id": "e7e39614",
        "outputId": "b52fb12b-848c-4ac6-c08b-5406751d074e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "6 Iceberg tables already exist -- no action will be taken on these...\n",
            "['students.hive2ice.tpch_cust_ice_orc', 'students.hive2ice.tpch_cust_ice_avro', 'students.hive2ice.tpch_orders_ice_orc', 'students.hive2ice.tpch_cust_ice_parquet', 'students.hive2ice.tpch_orders_ice_avro', 'students.hive2ice.tpch_orders_ice_parquet']\n",
            "\n",
            "6 Hive tables targeted to be MIGRATED (in-place) to Iceberg...\n",
            "['students.hive2ice.tpch_orders_hive_orc', 'students.hive2ice.tpch_cust_hive_orc', 'students.hive2ice.tpch_orders_hive_parquet', 'students.hive2ice.tpch_orders_hive_avro', 'students.hive2ice.tpch_cust_hive_avro', 'students.hive2ice.tpch_cust_hive_parquet']\n",
            "\n",
            "6 Hive tables targeted to be REWRITTEN (ctas) to Iceberg...\n",
            "['students.hive2ice.tpch_nation_hive_textfile', 'students.hive2ice.tpch_orders_hive_json', 'students.hive2ice.tpch_orders_hive_textfile', 'students.hive2ice.tpch_cust_hive_textfile', 'students.hive2ice.tpch_cust_hive_json', 'students.hive2ice.tpch_nation_hive_json']\n",
            "\n",
            "2 non Iceberg or Hive tables exist -- no action will be taken on these (BUT Delta Lake TABLES COULD BE REWRITTEN)...\n",
            "['students.hive2ice.tpch_orders_delta', 'students.hive2ice.tpch_cust_delta']\n"
          ]
        }
      ],
      "source": [
        "# create some lists to separate the table types into\n",
        "hive_tables_2rewrite = list()\n",
        "hive_tables_2migrate = list()\n",
        "iceberg_tables = list()\n",
        "other_tables   = list()\n",
        "\n",
        "# look at the table create statement to determine which category to use\n",
        "for a_table in table_list:\n",
        "    table_name = a_table.table_name\n",
        "    crt_tb = session.sql('show create table ' + table_name).collect()\n",
        "    cts = crt_tb[0][\"Create Table\"]\n",
        "\n",
        "    if \"type = 'HIVE'\" in cts:\n",
        "        if \"format = 'PARQUET'\" in cts or \"format = 'ORC'\" in cts or \"format = 'AVRO'\" in cts:\n",
        "            hive_tables_2migrate.append(table_name)\n",
        "        else:\n",
        "            hive_tables_2rewrite.append(table_name)\n",
        "    elif \"type = 'ICEBERG'\" in cts:\n",
        "        iceberg_tables.append(table_name)\n",
        "    else:\n",
        "        other_tables.append(table_name)\n",
        "\n",
        "\n",
        "print(\"\\n\" + str(len(iceberg_tables)) + \" Iceberg tables already exist -- no action will be taken on these...\")\n",
        "print(iceberg_tables)\n",
        "\n",
        "print(\"\\n\" + str(len(hive_tables_2migrate)) + \" Hive tables targeted to be MIGRATED (in-place) to Iceberg...\")\n",
        "print(hive_tables_2migrate)\n",
        "\n",
        "print(\"\\n\" + str(len(hive_tables_2rewrite)) + \" Hive tables targeted to be REWRITTEN (ctas) to Iceberg...\")\n",
        "print(hive_tables_2rewrite)\n",
        "\n",
        "print(\"\\n\" + str(len(other_tables)) + \" non Iceberg or Hive tables exist -- no action will be taken on these \" + \\\n",
        "      \"(BUT Delta Lake TABLES COULD BE REWRITTEN)...\")\n",
        "print(other_tables)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "701f59de",
      "metadata": {
        "id": "701f59de"
      },
      "source": [
        "---\n",
        "## Begin migration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a90a7114",
      "metadata": {
        "id": "a90a7114"
      },
      "source": [
        "### Perform in-place migrations\n",
        "\n",
        "Run `ALTER TABLE table_name SET PROPERTIES type = 'ICEBERG'` on Hive tables backed by ORC, Parquet, or Avro file format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "02809ab6",
      "metadata": {
        "id": "02809ab6",
        "outputId": "8e3c61af-2e15-42da-bb16-48a699cdcb73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "+++++++++++++++++++++++++++++\n",
            "++++ IN-PLACE MIGRATIONS ++++\n",
            "+++++++++++++++++++++++++++++\n",
            "\n",
            "\n",
            "in-place migration > students.hive2ice.tpch_orders_hive_orc\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "in-place migration > students.hive2ice.tpch_cust_hive_orc\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "in-place migration > students.hive2ice.tpch_orders_hive_parquet\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "in-place migration > students.hive2ice.tpch_orders_hive_avro\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "in-place migration > students.hive2ice.tpch_cust_hive_avro\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "in-place migration > students.hive2ice.tpch_cust_hive_parquet\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\")\n",
        "print(\"+++++++++++++++++++++++++++++\")\n",
        "print(\"++++ IN-PLACE MIGRATIONS ++++\")\n",
        "print(\"+++++++++++++++++++++++++++++\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# NEED TO HANDLE ANY EXEPTIONS THAT MIGHT BE RAISED\n",
        "#  these would likely be caused from invalid data types for starters\n",
        "#  which once identified could be added to the shadow migration list\n",
        "\n",
        "for tbl in hive_tables_2migrate:\n",
        "    print(\"in-place migration > \" + tbl)\n",
        "    session.sql(\"ALTER TABLE \" + tbl + \" SET PROPERTIES type = 'ICEBERG'\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eae5f327",
      "metadata": {
        "id": "eae5f327"
      },
      "source": [
        "### Perform shadow migrations\n",
        "\n",
        "Perform the following steps for Hive tables backed by file formats not supported directly by Iceberg.\n",
        "\n",
        "- `ALTER TABLE table_name RENAME TO hold_name`\n",
        "- `CREATE TABLE table_name WITH (with_props) AS SELECT * FROM hold_name`\n",
        "- `ALTER TABLE hold_name RENAME TO rm_name`\n",
        "- Add `rm_name` to a collection to later be dropped, if desired"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "dea61d52",
      "metadata": {
        "id": "dea61d52",
        "outputId": "d685b79a-c77b-4353-99b8-3fdf7d6490bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "+++++++++++++++++++++++++++++\n",
            "++++ SHADOW MIGRATIONS ++++++\n",
            "+++++++++++++++++++++++++++++\n",
            "+++ with_props > type='iceberg', format='parquet'\n",
            "+++++++++++++++++++++++++++++\n",
            "\n",
            "\n",
            "******** shadow migration > students.hive2ice.tpch_nation_hive_textfile\n",
            "\n",
            "ALTER TABLE students.hive2ice.tpch_nation_hive_textfile RENAME TO students.hive2ice.hold_tpch_nation_hive_textfile\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "CREATE TABLE students.hive2ice.tpch_nation_hive_textfile WITH (type='iceberg', format='parquet') AS SELECT * FROM students.hive2ice.hold_tpch_nation_hive_textfile\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "ALTER TABLE students.hive2ice.hold_tpch_nation_hive_textfile RENAME TO students.hive2ice.rm_hold_tpch_nation_hive_textfile\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "******** shadow migration > students.hive2ice.tpch_orders_hive_json\n",
            "\n",
            "ALTER TABLE students.hive2ice.tpch_orders_hive_json RENAME TO students.hive2ice.hold_tpch_orders_hive_json\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "CREATE TABLE students.hive2ice.tpch_orders_hive_json WITH (type='iceberg', format='parquet') AS SELECT * FROM students.hive2ice.hold_tpch_orders_hive_json\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "ALTER TABLE students.hive2ice.hold_tpch_orders_hive_json RENAME TO students.hive2ice.rm_hold_tpch_orders_hive_json\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "******** shadow migration > students.hive2ice.tpch_orders_hive_textfile\n",
            "\n",
            "ALTER TABLE students.hive2ice.tpch_orders_hive_textfile RENAME TO students.hive2ice.hold_tpch_orders_hive_textfile\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "CREATE TABLE students.hive2ice.tpch_orders_hive_textfile WITH (type='iceberg', format='parquet') AS SELECT * FROM students.hive2ice.hold_tpch_orders_hive_textfile\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "ALTER TABLE students.hive2ice.hold_tpch_orders_hive_textfile RENAME TO students.hive2ice.rm_hold_tpch_orders_hive_textfile\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "******** shadow migration > students.hive2ice.tpch_cust_hive_textfile\n",
            "\n",
            "ALTER TABLE students.hive2ice.tpch_cust_hive_textfile RENAME TO students.hive2ice.hold_tpch_cust_hive_textfile\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "CREATE TABLE students.hive2ice.tpch_cust_hive_textfile WITH (type='iceberg', format='parquet') AS SELECT * FROM students.hive2ice.hold_tpch_cust_hive_textfile\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "ALTER TABLE students.hive2ice.hold_tpch_cust_hive_textfile RENAME TO students.hive2ice.rm_hold_tpch_cust_hive_textfile\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "******** shadow migration > students.hive2ice.tpch_cust_hive_json\n",
            "\n",
            "ALTER TABLE students.hive2ice.tpch_cust_hive_json RENAME TO students.hive2ice.hold_tpch_cust_hive_json\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "CREATE TABLE students.hive2ice.tpch_cust_hive_json WITH (type='iceberg', format='parquet') AS SELECT * FROM students.hive2ice.hold_tpch_cust_hive_json\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "ALTER TABLE students.hive2ice.hold_tpch_cust_hive_json RENAME TO students.hive2ice.rm_hold_tpch_cust_hive_json\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "******** shadow migration > students.hive2ice.tpch_nation_hive_json\n",
            "\n",
            "ALTER TABLE students.hive2ice.tpch_nation_hive_json RENAME TO students.hive2ice.hold_tpch_nation_hive_json\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "CREATE TABLE students.hive2ice.tpch_nation_hive_json WITH (type='iceberg', format='parquet') AS SELECT * FROM students.hive2ice.hold_tpch_nation_hive_json\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "ALTER TABLE students.hive2ice.hold_tpch_nation_hive_json RENAME TO students.hive2ice.rm_hold_tpch_nation_hive_json\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\")\n",
        "print(\"+++++++++++++++++++++++++++++\")\n",
        "print(\"++++ SHADOW MIGRATIONS ++++++\")\n",
        "print(\"+++++++++++++++++++++++++++++\")\n",
        "print(\"+++ with_props > \" + with_props)\n",
        "print(\"+++++++++++++++++++++++++++++\")\n",
        "print(\"\\n\")\n",
        "\n",
        "old_tbls2rm = list()\n",
        "\n",
        "for tbl in hive_tables_2rewrite:\n",
        "    print(\"******** shadow migration > \" + tbl + \"\\n\")\n",
        "    fqtn = tbl.split(\".\")\n",
        "    hold_name = fqtn[0]+\".\"+fqtn[1]+\".hold_\"+fqtn[2]\n",
        "    alter2hold_cmd = \"ALTER TABLE \" + tbl + \" RENAME TO \" + hold_name\n",
        "    print(alter2hold_cmd)\n",
        "    session.sql(alter2hold_cmd).show()\n",
        "\n",
        "    # NEED TO TACKLE PARTITIONS BY LOOPING THROUGH THEM INSERTING FROM MOST RECENT TO LEAST RECENT\n",
        "    #  could even create a union with the old and new table then once new partition is committed,\n",
        "    #  quickly drop the old partition (briefly have 2 copies of each partition!!)\n",
        "\n",
        "    ctas_cmd = \"CREATE TABLE \" + tbl + \" WITH (\" + with_props + \") AS SELECT * FROM \" + hold_name\n",
        "    print(ctas_cmd)\n",
        "    session.sql(ctas_cmd).show()\n",
        "\n",
        "    fqtn = hold_name.split(\".\")\n",
        "    rm_name = fqtn[0]+\".\"+fqtn[1]+\".rm_\"+fqtn[2]\n",
        "    alter2rm_cmd = \"ALTER TABLE \" + hold_name + \" RENAME TO \" + rm_name\n",
        "    print(alter2rm_cmd)\n",
        "    session.sql(alter2rm_cmd).show()\n",
        "\n",
        "    # hold on to the rm_name for possible deletions later\n",
        "    old_tbls2rm.append(rm_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e85def2e",
      "metadata": {
        "id": "e85def2e"
      },
      "source": [
        "### Optionally, delete migrated tables\n",
        "\n",
        "If desired, run `DROP TABLE` commands on the original Hive tables that were migrated with the shadow (i.e. CTAS) approach.\n",
        "\n",
        "NOTE: Their names are prefixed with `rm_hold_`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9f965fe5",
      "metadata": {
        "id": "9f965fe5",
        "outputId": "836e3aea-5240-4a81-d0e8-6c4acd16938d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "dropping original table > students.hive2ice.rm_hold_tpch_nation_hive_textfile\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "\n",
            "dropping original table > students.hive2ice.rm_hold_tpch_orders_hive_json\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "\n",
            "dropping original table > students.hive2ice.rm_hold_tpch_orders_hive_textfile\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "\n",
            "dropping original table > students.hive2ice.rm_hold_tpch_cust_hive_textfile\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "\n",
            "dropping original table > students.hive2ice.rm_hold_tpch_cust_hive_json\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n",
            "\n",
            "dropping original table > students.hive2ice.rm_hold_tpch_nation_hive_json\n",
            "----------\n",
            "|status  |\n",
            "----------\n",
            "|ok      |\n",
            "----------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# cleanup of the original tables that were shadow migrated\n",
        "\n",
        "for tbl in old_tbls2rm:\n",
        "    print(\"\\ndropping original table > \" + tbl)\n",
        "    session.sql(\"DROP TABLE \" + tbl).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2ae11af",
      "metadata": {
        "id": "f2ae11af"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}