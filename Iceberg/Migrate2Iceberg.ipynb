{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3883d2a",
   "metadata": {},
   "source": [
    "# Starburst Galaxy Iceberg migration tool\n",
    "\n",
    "An interactive notebook used to migrate non-Iceberg tables in a given \n",
    "Starburst Galaxy data lake catalog.\n",
    "\n",
    "See [Migrate Hive tables to Apache Iceberg with Starburst Galaxy tutorial](https://www.starburst.io/tutorials/migrate-hive-tables-to-iceberg-with-starburst-galaxy/#0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8adb40",
   "metadata": {},
   "source": [
    "---\n",
    "## Config & setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b425ab",
   "metadata": {},
   "source": [
    "### Galaxy cluster & user credentials\n",
    "\n",
    "Run the next cell, but realize that it does NOT actually validate your values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ddb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "# grab credentials from the notebook user to be used when making a connection\n",
    "host = input(\"Host name\")\n",
    "username = input(\"User name\")\n",
    "password = getpass.getpass(\"Password\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f92b65",
   "metadata": {},
   "source": [
    "### Migration process parameters\n",
    "\n",
    "Update the values below to the most appropriate values for your migration effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f97beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Galaxy target catalog to perform migration on\n",
    "tgt_cat = 'mycatalog'\n",
    "\n",
    "# schema to target migration effort on  \n",
    "tgt_sch = 'myschema'\n",
    "#TODO: allow '*' to be valid and used to loop through all schemas in tgt_cat\n",
    "\n",
    "# CTAS properties used in WITH clause on new tables created for shadow migration\n",
    "with_props = \"type='iceberg', format='orc'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f55111",
   "metadata": {},
   "source": [
    "### Setup PyStarburst session\n",
    "\n",
    "Should return `[Row(Working='Yes')]` if functional.  If an exception is raised, \n",
    "it is likely due to incorrect cluster and/or credentials values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d31b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystarburst import Session\n",
    "from pystarburst import functions as F\n",
    "from pystarburst.functions import *\n",
    "from pystarburst.window import Window as W\n",
    "\n",
    "# PyStarburst setup\n",
    "session_properties = {\n",
    "    \"host\":host,\n",
    "    \"port\": 443,\n",
    "    \"http_scheme\": \"https\",\n",
    "    \"auth\": trino.auth.BasicAuthentication(username, password)\n",
    "}\n",
    "session = Session.builder.configs(session_properties).create()\n",
    "\n",
    "# validate PyStarburst working\n",
    "session.sql(\"select 'Yes' as Working\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2bd3d7",
   "metadata": {},
   "source": [
    "### Setup trino-python-client connection\n",
    "\n",
    "Should return `[['Yes, Working']]` if functional.  If an exception is raised, \n",
    "it is likely due to incorrect cluster and/or credentials values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc615ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trino.dbapi import connect\n",
    "\n",
    "# trino-python-client setup\n",
    "conn = connect(\n",
    "    host=host,\n",
    "    port=443,\n",
    "    http_scheme='https',\n",
    "    auth=trino.auth.BasicAuthentication (username, password)\n",
    ")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"select 'Yes, Working' as dummy\")\n",
    "\n",
    "# validate trino-python-client working\n",
    "print(cur.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8c4d62",
   "metadata": {},
   "source": [
    "---\n",
    "## Initial analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313d1add",
   "metadata": {},
   "source": [
    "### Identify targeted tables\n",
    "\n",
    "Fully qualified table names of all tables that will be interogated and attempted to be migrated if appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3c98b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all BASE TABLE entries in the info schema's tables table\n",
    "table_list_from_collect = session \\\n",
    "    .table(tgt_cat + \".information_schema.tables\") \\\n",
    "    .filter(\"table_schema = '\" + tgt_sch + \"' AND table_type = 'BASE TABLE'\") \\\n",
    "    .select(\"table_name\").sort(\"table_name\") \\\n",
    "    .collect()\n",
    "\n",
    "table_list = list()\n",
    "for a_table in table_list_from_collect:\n",
    "    full_name = tgt_cat + \".\" + tgt_sch + \".\" + a_table.table_name\n",
    "    table_list.append(full_name)\n",
    "    print(full_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e463cfb7",
   "metadata": {},
   "source": [
    "### Categorize tables\n",
    "\n",
    "Identify tables for each of the following categories.\n",
    "\n",
    "- Existing Iceberg tables (no action to take)\n",
    "- Hive tables backed by ORC, Parquet, or Avro (will attempt in-place migrations via ALTER command)\n",
    "- Hive tables backed by other file formats (will attempt shadow migrations via CTAS statement)\n",
    "- Non Hive or Iceberg tables (doing nothing with these at this time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e39614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some lists to separate the table types into\n",
    "hive_tables_2rewrite = list()\n",
    "hive_tables_2migrate = list()\n",
    "iceberg_tables = list()\n",
    "other_tables   = list()\n",
    "\n",
    "# could NOT get the SHOW CREATE TABLE output via PyStarburst \n",
    "#  OR figure out how to get the table format type in other way\n",
    "#  SO using trino-python-client\n",
    "\n",
    "for a_table in table_list:\n",
    "    cur.execute(\"SHOW CREATE TABLE \" + a_table)\n",
    "    cts = cur.fetchall()[0][0]\n",
    "    \n",
    "    if \"type = 'HIVE'\" in cts:\n",
    "        if \"format = 'PARQUET'\" in cts or \"format = 'ORC'\" in cts or \"format = 'AVRO'\" in cts:\n",
    "            hive_tables_2migrate.append(a_table)\n",
    "        else:\n",
    "            hive_tables_2rewrite.append(a_table)\n",
    "    elif \"type = 'ICEBERG'\" in cts:\n",
    "        iceberg_tables.append(a_table)\n",
    "    else:\n",
    "        other_tables.append(a_table)\n",
    "        \n",
    "        \n",
    "print(\"\\n\" + str(len(iceberg_tables)) + \" Iceberg tables already exist -- no action will be taken on these...\")\n",
    "print(iceberg_tables)\n",
    "\n",
    "print(\"\\n\" + str(len(hive_tables_2migrate)) + \" Hive tables targeted to be MIGRATED (in-place) to Iceberg...\")\n",
    "print(hive_tables_2migrate)\n",
    "\n",
    "print(\"\\n\" + str(len(hive_tables_2rewrite)) + \" Hive tables targeted to be REWRITTEN (ctas) to Iceberg...\")\n",
    "print(hive_tables_2rewrite)\n",
    "\n",
    "print(\"\\n\" + str(len(other_tables)) + \" non Iceberg or Hive tables exist -- no action will be taken on these \" + \\\n",
    "      \"(BUT Delta Lake TABLES COULD BE REWRITTEN)...\")\n",
    "print(other_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701f59de",
   "metadata": {},
   "source": [
    "---\n",
    "## Begin migration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90a7114",
   "metadata": {},
   "source": [
    "### Perform in-place migrations\n",
    "\n",
    "Run `ALTER TABLE table_name SET PROPERTIES type = 'ICEBERG'` on Hive tables backed by ORC, Parquet, or Avro file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02809ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "print(\"+++++++++++++++++++++++++++++\")\n",
    "print(\"++++ IN-PLACE MIGRATIONS ++++\")\n",
    "print(\"+++++++++++++++++++++++++++++\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# NEED TO HANDLE ANY EXEPTIONS THAT MIGHT BE RAISED\n",
    "#  these would likely be caused from invalid data types for starters\n",
    "#  which once identified could be added to the shadow migration list\n",
    "\n",
    "for tbl in hive_tables_2migrate:\n",
    "    print(\"in-place migration > \" + tbl)\n",
    "    session.sql(\"ALTER TABLE \" + tbl + \" SET PROPERTIES type = 'ICEBERG'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae5f327",
   "metadata": {},
   "source": [
    "### Perform shadow migrations\n",
    "\n",
    "Perform the following steps for Hive tables backed by file formats not supported directly by Iceberg.\n",
    "\n",
    "- `ALTER TABLE table_name RENAME TO hold_name`\n",
    "- `CREATE TABLE table_name WITH (with_props) AS SELECT * FROM hold_name`\n",
    "- `ALTER TABLE hold_name RENAME TO rm_name`\n",
    "- Add `rm_name` to a collection to later be dropped, if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea61d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "print(\"+++++++++++++++++++++++++++++\")\n",
    "print(\"++++ SHADOW MIGRATIONS ++++++\")\n",
    "print(\"+++++++++++++++++++++++++++++\")\n",
    "print(\"+++ with_props > \" + with_props)\n",
    "print(\"+++++++++++++++++++++++++++++\")\n",
    "print(\"\\n\")\n",
    "\n",
    "old_tbls2rm = list()\n",
    "\n",
    "for tbl in hive_tables_2rewrite:\n",
    "    print(\"******** shadow migration > \" + tbl + \"\\n\")\n",
    "    fqtn = tbl.split(\".\")\n",
    "    hold_name = fqtn[0]+\".\"+fqtn[1]+\".hold_\"+fqtn[2]\n",
    "    alter2hold_cmd = \"ALTER TABLE \" + tbl + \" RENAME TO \" + hold_name\n",
    "    print(alter2hold_cmd)\n",
    "    session.sql(alter2hold_cmd).show()\n",
    "    \n",
    "    # NEED TO TACKLE PARTITIONS BY LOOPING THROUGH THEM INSERTING FROM MOST RECENT TO LEAST RECENT\n",
    "    #  could even create a union with the old and new table then once new partition is committed,\n",
    "    #  quickly drop the old partition (briefly have 2 copies of each partition!!)\n",
    "\n",
    "    ctas_cmd = \"CREATE TABLE \" + tbl + \" WITH (\" + with_props + \") AS SELECT * FROM \" + hold_name\n",
    "    print(ctas_cmd)\n",
    "    session.sql(ctas_cmd).show()\n",
    "    \n",
    "    fqtn = hold_name.split(\".\")\n",
    "    rm_name = fqtn[0]+\".\"+fqtn[1]+\".rm_\"+fqtn[2]\n",
    "    alter2rm_cmd = \"ALTER TABLE \" + hold_name + \" RENAME TO \" + rm_name\n",
    "    print(alter2rm_cmd)\n",
    "    session.sql(alter2rm_cmd).show()\n",
    "    \n",
    "    # hold on to the rm_name for possible deletions later\n",
    "    old_tbls2rm.append(rm_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85def2e",
   "metadata": {},
   "source": [
    "### Optionally, delete migrated tables\n",
    "\n",
    "If desired, run `DROP TABLE` commands on the original Hive tables that were migrated with the shadow (i.e. CTAS) approach.\n",
    "\n",
    "NOTE: Their names are prefixed with `rm_hold_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f965fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup of the original tables that were shadow migrated\n",
    "\n",
    "for tbl in old_tbls2rm:\n",
    "    print(\"\\ndropping original table > \" + tbl)\n",
    "    session.sql(\"DROP TABLE \" + tbl).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ae11af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
