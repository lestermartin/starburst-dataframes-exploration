{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iox_ufgbqDXa"
      },
      "source": [
        "<h1><center>Initial smoke test of the Dell Data Analytics Engine (powered by Starburst)</center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGbIBPLHqVXc"
      },
      "source": [
        "<a id='the-runtime-environment'></a>\n",
        "## The runtime environment\n",
        "\n",
        "This notebook is to allow quick validation that\n",
        "[Apache Spark](https://spark.apache.org/) code can be run on the\n",
        "[Dell Data Analytics Engine](https://dell.starburst.io/latest/index.html) -- *powered by [Starburst](httphttps://www.starburst.io/s://)*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd6t0uFzuR4X"
      },
      "source": [
        "<a id='installing-spark'></a>\n",
        "## Installing Spark\n",
        "\n",
        "> These instructions where lifted & enhanced from [Colab and PySpark](https://colab.research.google.com/drive/1G894WS7ltIUTusWWmsCnF_zQhQqZCDOc) whose source file can be downloaded from [here](https://github.com/jacobceles/knowledge-repo/blob/master/pyspark/Colab%20and%20PySpark.ipynb) and then used with any Jupyter notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6apGVff5h4ca"
      },
      "source": [
        "Install Dependencies:\n",
        "\n",
        "1.   Java 8 (Dell appliance requires 22, but so far 8 is working from the notebook)\n",
        "2.   Apache Spark with hadoop (Settled on 3.5.1 for starters as needed >= 3.4 for Spark Connect)\n",
        "3.   Findspark (used to locate the spark in the system)\n",
        "\n",
        "> If you have issues with spark version, please upgrade to the latest version from [here](https://archive.apache.org/dist/spark/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt7ZS1_wGgjn"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ACYMwhgHTYz"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3x0ZRLxjMVr"
      },
      "source": [
        "Set Environment Variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdOOq4twHN1K"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='dell-cli-tasks'></a>\n",
        "## Dell CLI tasks\n",
        "\n"
      ],
      "metadata": {
        "id": "K_JmX0byk4l8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtain & set Spark Connect uri:\n",
        "\n",
        "> Full details in the [CLI docs](https://dell.starburst.io/latest/dell-data-processing-engine/cli.html),\n",
        "but here are the general steps after installation.\n",
        "\n",
        "Run the following wherever you have the Dell CLI installed.\n",
        "\n",
        "`./dell-data-processing-engine login`\n",
        "\n",
        "Replace `ACCESS_KEY` and `SECRET_KEY` accordingly and create the Spark Connect instance\n",
        "\n",
        "```\n",
        "./dell-data-processing-engine submit \\\n",
        "\t--conf spark.hadoop.fs.s3a.access.key=ACCESS_KEY \\\n",
        "\t--conf spark.hadoop.fs.s3a.secret.key=SECRET_KEY \\\n",
        "\t--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \\\n",
        "\t--conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider \\\n",
        "\t--conf spark.hadoop.fs.s3a.endpoint= \\\n",
        "\t--conf spark.sql.repl.eagerEval.enabled=True \\\n",
        "\t--spark-connect\n",
        "```\n",
        "\n",
        "Copy the outputted `sparkId` value to your clipboard and replace that with `REPLACE-ME` in next step\n",
        "\n",
        "`./dell-data-processing-engine instance uris REPLACE-ME`\n",
        "\n",
        "Copy the `Spark Connect` uri (starts with `sc://`) to your clipboard and use it in the next code cell\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Note: when all done be sure to run `./dell-data-processing-engine instance delete REPLACE-ME`**\n"
      ],
      "metadata": {
        "id": "JUt1XWkxTf4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# run this cell and past the Spark Connect uri in the textbox that surfaces (and press <enter> OF COURSE; haha)\n",
        "#\n",
        "\n",
        "import getpass\n",
        "\n",
        "sparkConnectUri = input(\"Spark Connect uri\")"
      ],
      "metadata": {
        "id": "FsEapqB8ObXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='run-spark'></a>\n",
        "## Run Spark\n"
      ],
      "metadata": {
        "id": "FZpDy0Y1lMZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the SparkSession:\n",
        "\n",
        "> Output should look similar to\n",
        "`<pyspark.sql.connect.session.SparkSession at 0x7fe9f73bbe90>`"
      ],
      "metadata": {
        "id": "GzO3UlGRY2NC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR1zLBk1998Z"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .remote(sparkConnectUri) \\\n",
        "    .getOrCreate()\n",
        "spark.version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a DataFrame from hard-coded data and display it:"
      ],
      "metadata": {
        "id": "YaZHv75wbpDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, date\n",
        "from pyspark.sql import Row\n",
        "\n",
        "bogus_df = spark.createDataFrame([\n",
        "  Row(aNbr=1, nutherNbr=2, aString='string1', aDate=date(2000, 1, 1), aTimestamp=datetime(2000, 1, 1, 12, 0)),\n",
        "  Row(aNbr=2, nutherNbr=3, aString='string2', aDate=date(2000, 2, 1), aTimestamp=datetime(2000, 1, 2, 12, 0)),\n",
        "  Row(aNbr=4, nutherNbr=5, aString='string3', aDate=date(2000, 3, 1), aTimestamp=datetime(2000, 1, 3, 12, 0)),\n",
        "  Row(aNbr=8, nutherNbr=7, aString='string4', aDate=date(2000, 4, 1), aTimestamp=datetime(2000, 1, 4, 12, 0)),\n",
        "])\n",
        "bogus_df.show()"
      ],
      "metadata": {
        "id": "P7fx7qSEbzg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Are you done?\n",
        "\n",
        "If so (or when you are), don't forget to run the following command.\n",
        "\n",
        "**`./dell-data-processing-engine instance delete REPLACE-ME`**\n"
      ],
      "metadata": {
        "id": "38TbF6rAj640"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmIqq6xPK7m7"
      },
      "source": [
        "<a id='transformation-logic'></a>\n",
        "## Transformation logic\n",
        "\n",
        "We are using the publicly available Bluebikes - Hubway dataset. Read more information [about Blue Bikes Boston](http://bluebikes.com/about), a bicycle-sharing program based in Boston since 2011.\n",
        "\n",
        "We are focusing on the [transactional records](https://bluebikes.com/system-data) of the bike trips from start to finish."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='exploring-the-raw-data'></a>\n",
        "### Exploring the raw data"
      ],
      "metadata": {
        "id": "k4xY8e2n4ipZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lets just grab a single CSV to explore with\n",
        "s3_file_path = \"s3a://starburst101-handsonlab-nyc-uber-rides/blue_bikes/raw_trips-2022_01-2022-09/202201-bluebikes-tripdata.csv\"\n",
        "\n",
        "# read CSV file into a DataFrame\n",
        "df = spark.read.csv(s3_file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "id": "W1llJIUd7mLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q: how many rows\n",
        "df.count()\n",
        "\n",
        "# RAISES EXCEPTION -- DON'T RUN!!\n",
        "#  Jordan is submitting a bug on this (4/23/2025)"
      ],
      "metadata": {
        "id": "22miG8TbH6eG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q: any null values?\n",
        "from pyspark.sql.functions import col\n",
        "df.filter(col(\"tripduration\").isNull()).show()\n",
        "\n",
        "# A: no null values found (that's good!)"
      ],
      "metadata": {
        "id": "CDCiFBXIaSGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q: tripduration values seem realistic? note: time is in seconds\n",
        "from pyspark.sql.functions import min, max, avg, count\n",
        "df.select(count(\"tripduration\"),\n",
        "                 min(\"tripduration\"),\n",
        "                 max(\"tripduration\"),\n",
        "                 avg(\"tripduration\")\n",
        "          ).show()\n",
        "\n",
        "# A: min trip is a minute seems ok, but max trip of 27 DAYS **seems** WRONG,\n",
        "#     but maybe this rider just didn't check the bike back in for a month\n",
        "#     and average of 20 minutes seems reasonable"
      ],
      "metadata": {
        "id": "MywY11XJFOAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q: are there a BUNCH of super long rides? Say greater than 16 hours (kept it with you all day)\n",
        "\n",
        "df.filter(\"tripduration > 50400\").sort(\"tripduration\", ascending=False).show(200)\n",
        "\n",
        "#df.filter(\"tripduration > 57600\").select(count(\"tripduration\")).show()\n",
        "\n",
        "# A: swap the comments on the lines above to see < 100 are (out of 81613 identified in prior cell),\n",
        "#     seems reasonable"
      ],
      "metadata": {
        "id": "L8n2Bw98cMyb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}