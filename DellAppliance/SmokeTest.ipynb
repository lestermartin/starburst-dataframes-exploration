{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lestermartin/starburst-dataframes-exploration/blob/main/DellAppliance/SmokeTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iox_ufgbqDXa"
      },
      "source": [
        "<h1><center>Initial smoke test of the Dell Data Analytics Engine (powered by Starburst)</center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGbIBPLHqVXc"
      },
      "source": [
        "<a id='the-runtime-environment'></a>\n",
        "## The runtime environment\n",
        "\n",
        "This notebook is to allow quick validation that\n",
        "[Apache Spark](https://spark.apache.org/) code can be run on the\n",
        "[Dell Data Analytics Engine](https://dell.starburst.io/latest/index.html) -- *powered by [Starburst](httphttps://www.starburst.io/s://)*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd6t0uFzuR4X"
      },
      "source": [
        "<a id='installing-spark'></a>\n",
        "## Installing Spark\n",
        "\n",
        "> These instructions where lifted & enhanced from [Colab and PySpark](https://colab.research.google.com/drive/1G894WS7ltIUTusWWmsCnF_zQhQqZCDOc) whose source file can be downloaded from [here](https://github.com/jacobceles/knowledge-repo/blob/master/pyspark/Colab%20and%20PySpark.ipynb) and then used with any Jupyter notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6apGVff5h4ca"
      },
      "source": [
        "Install Dependencies:\n",
        "\n",
        "1.   Java 8 (Dell appliance requires 22, but so far 8 is working from the notebook)\n",
        "2.   Apache Spark with hadoop (Settled on 3.5.1 for starters as needed >= 3.4 for Spark Connect)\n",
        "3.   Findspark (used to locate the spark in the system)\n",
        "\n",
        "> If you have issues with spark version, please upgrade to the latest version from [here](https://archive.apache.org/dist/spark/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt7ZS1_wGgjn"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ACYMwhgHTYz"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3x0ZRLxjMVr"
      },
      "source": [
        "Set Environment Variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdOOq4twHN1K"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='dell-cli-tasks'></a>\n",
        "## Dell CLI tasks\n",
        "\n"
      ],
      "metadata": {
        "id": "K_JmX0byk4l8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtain & set Spark Connect uri:\n",
        "\n",
        "> Full details in the [CLI docs](https://dell.starburst.io/latest/dell-data-processing-engine/cli.html),\n",
        "but here are the general steps after installation.\n",
        "\n",
        "Run the following wherever you have the Dell CLI installed.\n",
        "\n",
        "`./dell-data-processing-engine login`\n",
        "\n",
        "Replace `ACCESS_KEY` and `SECRET_KEY` accordingly and create the Spark Connect instance\n",
        "\n",
        "```\n",
        "./dell-data-processing-engine submit \\\n",
        "\t--conf spark.hadoop.fs.s3a.access.key=ACCESS_KEY \\\n",
        "\t--conf spark.hadoop.fs.s3a.secret.key=SECRET_KEY \\\n",
        "\t--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \\\n",
        "\t--conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider \\\n",
        "\t--conf spark.hadoop.fs.s3a.endpoint= \\\n",
        "\t--conf spark.sql.repl.eagerEval.enabled=True \\\n",
        "\t--spark-connect\n",
        "```\n",
        "\n",
        "Copy the outputted `sparkId` value to your clipboard and replace that with `REPLACE-ME` in next step\n",
        "\n",
        "`./dell-data-processing-engine instance uris REPLACE-ME`\n",
        "\n",
        "Copy the `Spark Connect` uri (starts with `sc://`) to your clipboard and use it in the next code cell\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Note: when all done be sure to run `./dell-data-processing-engine instance delete REPLACE-ME`**\n"
      ],
      "metadata": {
        "id": "JUt1XWkxTf4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# run this cell and past the Spark Connect uri in the textbox that surfaces (and press <enter> OF COURSE; haha)\n",
        "#\n",
        "\n",
        "import getpass\n",
        "\n",
        "sparkConnectUri = input(\"Spark Connect uri \")"
      ],
      "metadata": {
        "id": "FsEapqB8ObXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='run-spark'></a>\n",
        "## Run Spark\n"
      ],
      "metadata": {
        "id": "FZpDy0Y1lMZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the SparkSession:\n",
        "\n",
        "> Output should look similar to\n",
        "`<pyspark.sql.connect.session.SparkSession at 0x7fe9f73bbe90>`"
      ],
      "metadata": {
        "id": "GzO3UlGRY2NC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR1zLBk1998Z"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .remote(sparkConnectUri) \\\n",
        "    .getOrCreate()\n",
        "spark.version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a DataFrame from hard-coded data and display it:"
      ],
      "metadata": {
        "id": "YaZHv75wbpDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, date\n",
        "from pyspark.sql import Row\n",
        "\n",
        "bogus_df = spark.createDataFrame([\n",
        "  Row(aNbr=1, nutherNbr=2, aString='string1', aDate=date(2000, 1, 1), aTimestamp=datetime(2000, 1, 1, 12, 0)),\n",
        "  Row(aNbr=2, nutherNbr=3, aString='string2', aDate=date(2000, 2, 1), aTimestamp=datetime(2000, 1, 2, 12, 0)),\n",
        "  Row(aNbr=4, nutherNbr=5, aString='string3', aDate=date(2000, 3, 1), aTimestamp=datetime(2000, 1, 3, 12, 0)),\n",
        "  Row(aNbr=8, nutherNbr=7, aString='string4', aDate=date(2000, 4, 1), aTimestamp=datetime(2000, 1, 4, 12, 0)),\n",
        "])\n",
        "bogus_df.show()"
      ],
      "metadata": {
        "id": "P7fx7qSEbzg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can open up the Spark UI by getting the `Spark Web UI` link from the earlier run of `./dell-data-processing-engine instance uris REPLACE-ME` and opening it up in the same browser that launched when you ran the `login` command.\n",
        "\n",
        "If you navigate to the **SQL / Dataframe** tab you should see something similar to the following now.\n",
        "\n",
        "![alt text](https://github.com/lestermartin/starburst-dataframes-exploration/blob/main/DellAppliance/SparkUI.png?raw=true \"Spark Web UI screenshot\")\n"
      ],
      "metadata": {
        "id": "xODX5FuY2Laj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Are you done?\n",
        "\n",
        "If so (or when you are), don't forget to run the following command.\n",
        "\n",
        "**`./dell-data-processing-engine instance delete REPLACE-ME`**\n"
      ],
      "metadata": {
        "id": "38TbF6rAj640"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmIqq6xPK7m7"
      },
      "source": [
        "<a id='transformation-logic'></a>\n",
        "## Transformation logic\n",
        "\n",
        "We are using the publicly available Bluebikes - Hubway dataset. Read more information [about Blue Bikes Boston](http://bluebikes.com/about), a bicycle-sharing program based in Boston since 2011.\n",
        "\n",
        "We are focusing on the [transactional records](https://bluebikes.com/system-data) of the bike trips from start to finish."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='exploring-the-raw-data'></a>\n",
        "### Exploring the raw data"
      ],
      "metadata": {
        "id": "k4xY8e2n4ipZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lets just grab a single CSV to explore with\n",
        "s3_file_path = \"s3a://starburst101-handsonlab-nyc-uber-rides/blue_bikes/raw_trips-2022_01-2022-09/202201-bluebikes-tripdata.csv\"\n",
        "\n",
        "# read CSV file into a DataFrame\n",
        "df = spark.read.csv(s3_file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "id": "W1llJIUd7mLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q: how many rows\n",
        "df.count()\n",
        "\n",
        "# RAISES EXCEPTION -- DON'T RUN!!\n",
        "#  looks like captured in https://issues.apache.org/jira/browse/SPARK-45769"
      ],
      "metadata": {
        "id": "22miG8TbH6eG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we are going to explore 'df' several times and Spark uses lazy execution so let's just cache it\n",
        "#  NOTE: won't need this when turn this into a batch program\n",
        "\n",
        "from pyspark import StorageLevel\n",
        "\n",
        "df.cache()"
      ],
      "metadata": {
        "id": "34kLtXaqTMvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q: any null values?\n",
        "from pyspark.sql.functions import col\n",
        "df.filter(col(\"tripduration\").isNull()).show()\n",
        "\n",
        "# A: no null values found (that's good!)"
      ],
      "metadata": {
        "id": "CDCiFBXIaSGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q: tripduration values seem realistic? note: time is in seconds\n",
        "from pyspark.sql.functions import min, max, avg, count\n",
        "df.select(count(\"tripduration\"),\n",
        "                 min(\"tripduration\"),\n",
        "                 max(\"tripduration\"),\n",
        "                 avg(\"tripduration\")\n",
        "          ).show()\n",
        "\n",
        "# A: min trip is a minute seems ok, but max trip of 27 DAYS **seems** WRONG,\n",
        "#     but maybe this rider just didn't check the bike back in for a month\n",
        "#     and average of 20 minutes seems reasonable"
      ],
      "metadata": {
        "id": "MywY11XJFOAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q: are there a BUNCH of super long rides? Say greater than 16 hours (kept it with you all day)\n",
        "\n",
        "df.filter(\"tripduration > 50400\").sort(\"tripduration\", ascending=False).show(200)\n",
        "\n",
        "# A: well, it is less than 200 at least!"
      ],
      "metadata": {
        "id": "L8n2Bw98cMyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q: exactly how many are there greater than 16 hours?\n",
        "\n",
        "df.filter(\"tripduration > 50400\").select(count(\"tripduration\")).show()\n",
        "\n",
        "# A: Approx 100 out of 81613 seems reasonable"
      ],
      "metadata": {
        "id": "p6UsrcXsRqxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q: what is the exact count of rides longer than 18 hours (added 2 hours after) columns have concerning names and/or data types?\n",
        "\n",
        "df.printSchema()\n",
        "\n",
        "# A: the names are all slammed together or include spaces between the words\n",
        "#     fortunately, the data types look pretty good although we'll want to\n",
        "#     standardize the number of decimal places for the lat/long values"
      ],
      "metadata": {
        "id": "MZ9RvbqeCjs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# standardize the column names\n",
        "\n",
        "renames_df = df.withColumnRenamed('tripduration',            'trip_seconds') \\\n",
        "               .withColumnRenamed('starttime',               'start_time') \\\n",
        "               .withColumnRenamed('stoptime',                'stop_time') \\\n",
        "               .withColumnRenamed('start station id',        'start_station_id') \\\n",
        "               .withColumnRenamed('start station name',      'start_station_name') \\\n",
        "               .withColumnRenamed('start station latitude',  'start_station_latitude') \\\n",
        "               .withColumnRenamed('start station longitude', 'start_station_longitude') \\\n",
        "               .withColumnRenamed('end station id',          'end_station_id') \\\n",
        "               .withColumnRenamed('end station name',        'end_station_name') \\\n",
        "               .withColumnRenamed('end station latitude',    'end_station_latitude') \\\n",
        "               .withColumnRenamed('end station longitude',   'end_station_longitude') \\\n",
        "               .withColumnRenamed('bikeid',                  'bike_id') \\\n",
        "               .withColumnRenamed('usertype',                'user_type') \\\n",
        "               .withColumnRenamed('postal code',             'postal_code')\n",
        "renames_df.show(truncate=False)"
      ],
      "metadata": {
        "id": "VmDrhbhIEdDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cast the lat & long values to Decimal (15,13) and (16,13), respectively\n",
        "\n",
        "from pyspark.sql.types import DecimalType\n",
        "\n",
        "after_cast_df = renames_df.withColumn(\"start_station_latitude\", col(\"start_station_latitude\").cast(DecimalType(15, 13))) \\\n",
        "                          .withColumn(\"end_station_latitude\", col(\"end_station_latitude\").cast(DecimalType(15, 13))) \\\n",
        "                          .withColumn(\"start_station_longitude\", col(\"start_station_longitude\").cast(DecimalType(16, 13))) \\\n",
        "                          .withColumn(\"end_station_longitude\", col(\"end_station_longitude\").cast(DecimalType(16, 13)))\n",
        "after_cast_df.printSchema()"
      ],
      "metadata": {
        "id": "DNqpbIhaHzt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# notice the changes in the lat/long field values (some longer due to zeros and some rounded off to be shorter)\n",
        "\n",
        "after_cast_df.show()"
      ],
      "metadata": {
        "id": "wrb6Of_rKwVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# and, OF COURSE, we could have done all of this in one pass\n",
        "\n",
        "from pyspark.sql.types import DecimalType\n",
        "\n",
        "one_pass_df = df.withColumnRenamed('tripduration',            'trip_seconds') \\\n",
        "                .withColumnRenamed('starttime',               'start_time') \\\n",
        "                .withColumnRenamed('stoptime',                'stop_time') \\\n",
        "                .withColumnRenamed('start station id',        'start_station_id') \\\n",
        "                .withColumnRenamed('start station name',      'start_station_name') \\\n",
        "                .withColumnRenamed('start station latitude',  'start_station_latitude') \\\n",
        "                .withColumn(\"start_station_latitude\", col(\"start_station_latitude\").cast(DecimalType(15, 13))) \\\n",
        "                .withColumnRenamed('start station longitude', 'start_station_longitude') \\\n",
        "                .withColumn(\"start_station_longitude\", col(\"start_station_longitude\").cast(DecimalType(16, 13))) \\\n",
        "                .withColumnRenamed('end station id',          'end_station_id') \\\n",
        "                .withColumnRenamed('end station name',        'end_station_name') \\\n",
        "                .withColumnRenamed('end station latitude',    'end_station_latitude') \\\n",
        "                .withColumn(\"end_station_latitude\", col(\"end_station_latitude\").cast(DecimalType(15, 13))) \\\n",
        "                .withColumnRenamed('end station longitude',   'end_station_longitude') \\\n",
        "                .withColumn(\"end_station_longitude\", col(\"end_station_longitude\").cast(DecimalType(16, 13))) \\\n",
        "                .withColumnRenamed('bikeid',                  'bike_id') \\\n",
        "                .withColumnRenamed('usertype',                'user_type') \\\n",
        "                .withColumnRenamed('postal code',             'postal_code')\n",
        "one_pass_df.show(truncate=False)"
      ],
      "metadata": {
        "id": "yV5qSgbZM_VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='define-the-schema'></a>\n",
        "### Define the schema\n",
        "\n",
        "We COULD just use this approach of INFERRING THE SCHEMA IMPLICITLY, but there might be problems from file to file. An example might be how a header name on the first row might change in a future file which would break out transformations.\n",
        "\n",
        "Therefore, once we feel we have the schema figured out (including standardized naming and any data type conversions) we can DEFINE THE SCHEMA EXPLICITY to make our transformation job more robust."
      ],
      "metadata": {
        "id": "0Ojjup7fQdLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show the schema as we have it now\n",
        "one_pass_df.printSchema()"
      ],
      "metadata": {
        "id": "HHpR8mOZVp1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a list of the schema in the format column_name, data_type\n",
        "\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "labels = [\n",
        "     ('trip_seconds',IntegerType()),\n",
        "     ('start_time',TimestampType()),\n",
        "     ('stop_time',TimestampType()),\n",
        "     ('start_station_id',IntegerType()),\n",
        "     ('start_station_name',StringType()),\n",
        "     ('start_station_latitude',DecimalType(15,13)),\n",
        "     ('start_station_longitude',DecimalType(16,13)),\n",
        "     ('end_station_id',IntegerType()),\n",
        "     ('end_station_name',StringType()),\n",
        "     ('end_station_latitude',DecimalType(15,13)),\n",
        "     ('end_station_longitude',DecimalType(16,13)),\n",
        "     ('bike_id',IntegerType()),\n",
        "     ('user_type',StringType()),\n",
        "     ('postal_code',StringType())\n",
        "]\n",
        "\n",
        "# Creating the schema that will be passed when reading the csv\n",
        "\n",
        "schema = StructType([StructField (x[0], x[1], True) for x in labels])\n",
        "schema"
      ],
      "metadata": {
        "id": "MbpWt2hIQaBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets just grab a single CSV to explore with\n",
        "s3_file_path = \"s3a://starburst101-handsonlab-nyc-uber-rides/blue_bikes/raw_trips-2022_01-2022-09/202201-bluebikes-tripdata.csv\"\n",
        "\n",
        "# read CSV file into a DataFrame\n",
        "df_explicit_schema = spark.read.csv(s3_file_path, header=True, schema=schema)\n",
        "\n",
        "# Show the DataFrame\n",
        "df_explicit_schema.printSchema()"
      ],
      "metadata": {
        "id": "G8lQvXLhXwrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify data still looks good, too\n",
        "df_explicit_schema.show()"
      ],
      "metadata": {
        "id": "kuMnkc1KYnKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's just use SQL for a bit...\n",
        "df_explicit_schema.createOrReplaceTempView(\"raw_rides\")\n",
        "\n",
        "# quick check that the temp name works in SQL\n",
        "spark.sql(\"SELECT * FROM raw_rides\").show()"
      ],
      "metadata": {
        "id": "fmvfxrnnbvSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='clean-the-data'></a>\n",
        "### Clean the data\n",
        "\n",
        "For this demo we will just pretend we did the proper due diligence and found out that all of the columns, except for the last, were being received with high quality.\n",
        "\n",
        "We will just focus on the last column, postal_code; especially since there are hard-coded strings of 'NULL' noticeable in prior results.\n"
      ],
      "metadata": {
        "id": "y8H1pP2FI1fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q: does min/max tell us anything?\n",
        "\n",
        "spark.sql(\"select min(postal_code), max(postal_code) from raw_rides\").show()\n",
        "\n",
        "# A: the min of 00000 (which is actually in the CSV) indicates in invalid USA zip code\n",
        "#     and the 'W12 9PL' indicates there are customers who live in Canada (close-ish to Boston)"
      ],
      "metadata": {
        "id": "xD3n-OmrST2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q: how many with 00000?\n",
        "spark.sql(\"select count(*) from raw_rides where postal_code = '00000'\").show()\n",
        "\n",
        "# A: 126 (those would be better set to NULL)"
      ],
      "metadata": {
        "id": "egDtH4OJiQi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q: how many with empty strings or hard-coded to 'NULL'?\n",
        "\n",
        "spark.sql(\n",
        "    \"SELECT postal_code, count(*) \"\\\n",
        "    \"  FROM raw_rides \"\\\n",
        "    \" WHERE postal_code IN ('', 'NULL')\"\\\n",
        "    \" GROUP BY postal_code\").show()\n",
        "\n",
        "# A: seems only ones with hard-coded 'NULL' values (which should be true NULL values)"
      ],
      "metadata": {
        "id": "u4iMDTCejUZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q: any with strings that start with a space?\n",
        "\n",
        "spark.sql(\"select count(*) from raw_rides where postal_code LIKE ' %'\").show()\n",
        "\n",
        "# A: none"
      ],
      "metadata": {
        "id": "JdcIUKUcESep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q: are there some actual NULL values present?\n",
        "spark.sql(\"select count(*) from raw_rides where postal_code IS NULL\").show()\n",
        "\n",
        "# A: Yep, just over a 100 are present"
      ],
      "metadata": {
        "id": "xjswQ8q9jh7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the 3260 records with 'NULL' and the 126 with '00000' (total of 3386)\n",
        "#  to actual NULL values\n",
        "\n",
        "from pyspark.sql.functions import count\n",
        "\n",
        "df_cleaned = df_explicit_schema \\\n",
        "  .replace({'00000': None}, subset=['postal_code']) \\\n",
        "  .replace({'NULL': None}, subset=['postal_code'])\n",
        "\n",
        "# the total should now be the converted 3386 + the original 116 which is 3502\n",
        "\n",
        "df_cleaned.filter(\"postal_code IS NULL\").select(count(\"trip_seconds\")).show()"
      ],
      "metadata": {
        "id": "eWVNPB9M5Au2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='enrich-the-data'></a>\n",
        "### Enrich the data\n",
        "\n",
        "There is an enrichment requirement to augment the bike rides by adding province and average income values based on rider postal code.\n",
        "\n",
        "Explore the lookup dataset which only contains USA-based zip codes and focus on the columns than can be leveraged for our enrichment needs."
      ],
      "metadata": {
        "id": "0VGht2uPSUUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load and cache the lookup dataset\n",
        "\n",
        "# TODO: parameterize bucket name\n",
        "s3_file_path_for_lookup = \"s3a://starburst101-handsonlab-nyc-uber-rides/common/zip_code_income/\"\n",
        "\n",
        "lookup_labels = [\n",
        "     ('state',StringType()),\n",
        "     ('zip_code',IntegerType()),\n",
        "     ('num_returns',IntegerType()),\n",
        "     ('agi',IntegerType()),\n",
        "     ('agi_avg',DecimalType(16,8)),\n",
        "     ('num_returns_with_tot_inc',IntegerType()),\n",
        "     ('tot_inc_amt',IntegerType()),\n",
        "     ('tot_inc_avg',DecimalType(16,8)),\n",
        "     ('num_returns_with_tax_inc',IntegerType()),\n",
        "     ('tax_inc_amt',IntegerType()),\n",
        "     ('tax_inc_avg',DecimalType(16,8))\n",
        "]\n",
        "\n",
        "# Creating the schema that will be passed when reading the csv\n",
        "lookup_schema = StructType([StructField (x[0], x[1], True) for x in lookup_labels])\n",
        "\n",
        "# read CSV file into a DataFrame\n",
        "lookup_df = spark.read.csv(s3_file_path_for_lookup, header=False, schema=lookup_schema)\n",
        "\n",
        "# mark it to be cached (again, not needed for the batch version)\n",
        "lookup_df.cache()\n",
        "\n",
        "# make the df available to simple SQL\n",
        "lookup_df.createOrReplaceTempView(\"zip_code_income\")\n",
        "\n",
        "# Show the DataFrame\n",
        "lookup_df.show()"
      ],
      "metadata": {
        "id": "HUjwFWCjSnnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look at some values from around Roswell, Georgia\n",
        "\n",
        "spark.sql(\n",
        "    \"SELECT zip_code, state, tot_inc_avg * 1000 AS avg_income \"\\\n",
        "    \"  FROM zip_code_income \"\\\n",
        "    \" WHERE zip_code IN (30004, 30009, 30022, 30075, 30076, 30092)\").show()"
      ],
      "metadata": {
        "id": "RFxH2IrdMTfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# notice the zip_code is a numeric field, BUT giving it zip codes starting\n",
        "#  with 0 basically still works\n",
        "\n",
        "spark.sql(\n",
        "    \"SELECT * FROM zip_code_income \"\\\n",
        "    \" WHERE zip_code IN (02139, 02124)\").show()"
      ],
      "metadata": {
        "id": "iDNMC3KaQIZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify the join will work fine despite diff data types on the postal/zip code columns\n",
        "\n",
        "# this didn't happen yet, so making it available\n",
        "df_cleaned.createOrReplaceTempView(\"cleaned_rides\")\n",
        "\n",
        "spark.sql(\n",
        "    \"SELECT r.postal_code, z.tot_inc_amt \"\\\n",
        "    \"  FROM cleaned_rides AS r \"\\\n",
        "    \"  JOIN zip_code_income AS z ON (r.postal_code = z.zip_code) \"\\\n",
        "    \" WHERE postal_code IN ('02139', '02124')\").show()"
      ],
      "metadata": {
        "id": "0NVIcFDpQ1w3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a cleaned up & trimmed down df of the data to include in the join\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "min_cols_of_lookup_df = lookup_df.select(\"zip_code\", \"state\", \"tot_inc_avg\") \\\n",
        "  .withColumnRenamed(\"state\", \"province\") \\\n",
        "  .withColumn(\"tot_inc_avg\", col(\"tot_inc_avg\") * 1000) \\\n",
        "  .withColumnRenamed(\"tot_inc_avg\", \"avg_income\")\n",
        "\n",
        "min_cols_of_lookup_df.show()"
      ],
      "metadata": {
        "id": "vUUjRLhfUdNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# join the datasets and get rid of the extra zip code column\n",
        "df_enriched = df_cleaned.join(min_cols_of_lookup_df,\n",
        "                              df_cleaned.postal_code == min_cols_of_lookup_df.zip_code,\n",
        "                              \"left\") \\\n",
        "                        .drop(\"zip_code\")\n",
        "\n",
        "df_enriched.show()"
      ],
      "metadata": {
        "id": "H-1SVWmvXEiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check on the Canadian postal codes (which should have avg_income set to NULL)\n",
        "df_enriched.filter(\"postal_code IS NOT NULL\") \\\n",
        "           .select(\"postal_code\", \"province\", \"avg_income\") \\\n",
        "           .orderBy(\"postal_code\", ascending=False) \\\n",
        "           .show(50, truncate=False)"
      ],
      "metadata": {
        "id": "0ljuoLTFZBee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='persist-the-data'></a>\n",
        "### Persist the data\n",
        "\n",
        "Stopping here for now, but basically the next step is to save this as Parquet files to a destination that there is a Hive external table looking at.\n",
        "\n",
        "Once I can use the shared catalog, can do that to a managed table.\n",
        "\n",
        "EITHER WAY, this is our SILVER dataset.\n",
        "\n",
        "THEN I can use the Starburst UI and create some views for the GOLD layer."
      ],
      "metadata": {
        "id": "vi3qm5_OZ5bw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test it as batch\n",
        "\n",
        "The next cell is validating the short/n/sweet version that can then be saved in a .py file and submitted via the CLI."
      ],
      "metadata": {
        "id": "f7TT_dABamO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import col, count\n",
        "\n",
        "# initialize the application\n",
        "findspark.init()\n",
        "spark = SparkSession.builder \\\n",
        "    .remote(sparkConnectUri) \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# load the raw data with appropriate names and datatypes\n",
        "labels = [\n",
        "     ('trip_seconds',IntegerType()),\n",
        "     ('start_time',TimestampType()),\n",
        "     ('stop_time',TimestampType()),\n",
        "     ('start_station_id',IntegerType()),\n",
        "     ('start_station_name',StringType()),\n",
        "     ('start_station_latitude',DecimalType(15,13)),\n",
        "     ('start_station_longitude',DecimalType(16,13)),\n",
        "     ('end_station_id',IntegerType()),\n",
        "     ('end_station_name',StringType()),\n",
        "     ('end_station_latitude',DecimalType(15,13)),\n",
        "     ('end_station_longitude',DecimalType(16,13)),\n",
        "     ('bike_id',IntegerType()),\n",
        "     ('user_type',StringType()),\n",
        "     ('postal_code',StringType())\n",
        "]\n",
        "schema = StructType([StructField (x[0], x[1], True) for x in labels])\n",
        "\n",
        "# TODO: parameterize the bucket name AND the new file to process\n",
        "s3_file_path = \"s3a://starburst101-handsonlab-nyc-uber-rides/blue_bikes/raw_trips-2022_01-2022-09/202201-bluebikes-tripdata.csv\"\n",
        "df_explicit_schema = spark.read.csv(s3_file_path, header=True, schema=schema)\n",
        "\n",
        "# clean the dataset (our simple example only deals with postal_code)\n",
        "df_cleaned = df_explicit_schema \\\n",
        "  .replace({'00000': None}, subset=['postal_code']) \\\n",
        "  .replace({'NULL': None}, subset=['postal_code'])\n",
        "\n",
        "# load the lookup dataset\n",
        "lookup_labels = [\n",
        "     ('state',StringType()),\n",
        "     ('zip_code',IntegerType()),\n",
        "     ('num_returns',IntegerType()),\n",
        "     ('agi',IntegerType()),\n",
        "     ('agi_avg',DecimalType(16,8)),\n",
        "     ('num_returns_with_tot_inc',IntegerType()),\n",
        "     ('tot_inc_amt',IntegerType()),\n",
        "     ('tot_inc_avg',DecimalType(16,8)),\n",
        "     ('num_returns_with_tax_inc',IntegerType()),\n",
        "     ('tax_inc_amt',IntegerType()),\n",
        "     ('tax_inc_avg',DecimalType(16,8))\n",
        "]\n",
        "lookup_schema = StructType([StructField (x[0], x[1], True) for x in lookup_labels])\n",
        "\n",
        "# TODO: parameterize bucket name AND folder name\n",
        "s3_file_path_for_lookup = \"s3a://starburst101-handsonlab-nyc-uber-rides/common/zip_code_income/\"\n",
        "lookup_df = spark.read.csv(s3_file_path_for_lookup, header=False, schema=lookup_schema)\n",
        "\n",
        "# create a cleaned up & trimmed down df of the data to include in the join\n",
        "min_cols_of_lookup_df = lookup_df.select(\"zip_code\", \"state\", \"tot_inc_avg\") \\\n",
        "  .withColumnRenamed(\"state\", \"province\") \\\n",
        "  .withColumn(\"tot_inc_avg\", col(\"tot_inc_avg\") * 1000) \\\n",
        "  .withColumnRenamed(\"tot_inc_avg\", \"avg_income\")\n",
        "\n",
        "# join the datasets and get rid of the extra zip code column\n",
        "df_enriched = df_cleaned.join(min_cols_of_lookup_df,\n",
        "                              df_cleaned.postal_code == min_cols_of_lookup_df.zip_code,\n",
        "                              \"left\") \\\n",
        "                        .drop(\"zip_code\")\n",
        "\n",
        "# FOR NOW... JUST SHOW IT!\n",
        "df_enriched.show()"
      ],
      "metadata": {
        "id": "nfxptp1ia5d1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Are you done?\n",
        "\n",
        "If so (or when you are), don't forget to run the following command (the REPLACE-ME is explained at the top of this notebook).\n",
        "\n",
        "**`./dell-data-processing-engine instance delete REPLACE-ME`**"
      ],
      "metadata": {
        "id": "HHuDw92WFmIa"
      }
    }
  ]
}